{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment 2 for CSE 7324 Fall 2017\n",
    "\n",
    "___Members___: Hongning Yu, Hui Jiang, Hao Pan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Business Understanding\n",
    "The dataset we use is a lyrics dataset (lyrics from MetrLyrics), which can be downloaded from Kaggle for free: https://www.kaggle.com/gyani95/380000-lyrics-from-metrolyrics. By exploring this dataset, we are able to know the key features of certain song genre and predict the corresponding genre for new songs.\n",
    "\n",
    "\n",
    "In this dataset, there are 362237 records and 5 features (song name, year, artist, genre, and lyrics). It is comprised of text documents and contains only text divided into documents. Besides, we can predict song genres according to lyrics, so it meets requirements for Lab 2.\n",
    "\n",
    "\n",
    "For this project, our mainly purpose is to find the features for different song genres by analyzing the most frequent words in lyrics. And visualizing features will reveal more information about those features in the dataset. And then we may be able to figure out the relationship among features, which might benefit our genre prediction as well.\n",
    "\n",
    "\n",
    "The statictic and prediction results can be applied to applications related to song searching or displaying. For example, song searching applications, like Siri may use when you ask her \"What song is it?\", can narrow down song searching scope by classify songs according to lyric features. As for song displaying application, it could reconmend songs by analyzing lyrics from users' favorite songs.\n",
    "\n",
    "\n",
    "To ensure the correct rate of our prediction, we will keep a predict accuracy(AUC) target, like 80%, using accuracy measurement functions. We will use other more helpful evaluation metrics and functions if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Encoding\n",
    "First let's load the data in to dataframe. The data is already in a csv file but all of the lyrics are in raw text with different formats. Our gold is to predict genre basing on lyrics, so we still need to clean all lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "pd.set_option('display.max_columns', 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>song</th>\n",
       "      <th>year</th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ego-remix</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Oh baby, how you doing?\\nYou know I'm gonna cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>then-tell-me</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>playin' everything so easy,\\nit's like you see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>honesty</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>If you search\\nFor tenderness\\nIt isn't hard t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>you-are-my-rock</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Oh oh oh I, oh oh oh I\\n[Verse 1:]\\nIf I wrote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>black-culture</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Party the people, the people the party it's po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index             song  year           artist genre  \\\n",
       "0      0        ego-remix  2009  beyonce-knowles   Pop   \n",
       "1      1     then-tell-me  2009  beyonce-knowles   Pop   \n",
       "2      2          honesty  2009  beyonce-knowles   Pop   \n",
       "3      3  you-are-my-rock  2009  beyonce-knowles   Pop   \n",
       "4      4    black-culture  2009  beyonce-knowles   Pop   \n",
       "\n",
       "                                              lyrics  \n",
       "0  Oh baby, how you doing?\\nYou know I'm gonna cu...  \n",
       "1  playin' everything so easy,\\nit's like you see...  \n",
       "2  If you search\\nFor tenderness\\nIt isn't hard t...  \n",
       "3  Oh oh oh I, oh oh oh I\\n[Verse 1:]\\nIf I wrote...  \n",
       "4  Party the people, the people the party it's po...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./lyrics.csv\", encoding=\"utf-8\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check null values in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index         0\n",
       "song          2\n",
       "year          0\n",
       "artist        0\n",
       "genre         0\n",
       "lyrics    95680\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there are null values in lyrics and song. Just drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index     0\n",
       "song      0\n",
       "year      0\n",
       "artist    0\n",
       "genre     0\n",
       "lyrics    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rock             109235\n",
       "Pop               40466\n",
       "Hip-Hop           24850\n",
       "Not Available     23941\n",
       "Metal             23759\n",
       "Country           14387\n",
       "Jazz               7970\n",
       "Electronic         7966\n",
       "Other              5189\n",
       "R&B                3401\n",
       "Indie              3149\n",
       "Folk               2243\n",
       "Name: genre, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.genre.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, some genres have way more records than others. For our genre-predicting classification problem, we could sample the dataset and choose subsets of some genres to avoid bias. But let's now keep it as it is and deal with this later.\n",
    "\n",
    "Check certain genres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 266556 entries, 0 to 362236\n",
      "Data columns (total 6 columns):\n",
      "index     266556 non-null int64\n",
      "song      266556 non-null object\n",
      "year      266556 non-null int64\n",
      "artist    266556 non-null object\n",
      "genre     266556 non-null object\n",
      "lyrics    266556 non-null object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 14.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Read in data and check data quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change to ASCII\n",
    "First let's try to get rid of all non-ascii characters, since we only want english characters\n",
    "\n",
    "**Takes too much time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# import re\n",
    "# for row in df.index[:1000]:\n",
    "#     df.loc[row, 'lyrics'] = df.loc[row, 'lyrics'].encode('ascii', errors='ignore').decode()\n",
    "\n",
    "# for row in df.index[:1000]:\n",
    "#     df.loc[row, 'lyrics'] = re.sub(r'[^\\x00-\\x7f]',\n",
    "#                                    r'', \n",
    "#                                    df.loc[row, 'lyrics']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English Filter\n",
    "We want to focus on song's with english lyrics, so let's delete all non-english records if they exist.\n",
    "\n",
    "I tried to build a English-ratio detector to eliminate all non-english songs. \n",
    "Reference: https://github.com/rasbt/musicmood/blob/master/code/collect_data/data_collection.ipynb\n",
    "\n",
    "But the loop of set calculation **takes too much time**. Need to improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# def eng_ratio(text):\n",
    "#     ''' Returns the ratio of non-English to English words from a text '''\n",
    "\n",
    "#     english_vocab = set(w.lower() for w in nltk.corpus.words.words()) \n",
    "#     text_vocab = set(w.lower() for w in text.split('-') if w.lower().isalpha()) \n",
    "#     unusual = text_vocab.difference(english_vocab)\n",
    "#     diff = len(unusual)/(len(text_vocab)+1)\n",
    "#     return diff\n",
    "\n",
    "    \n",
    "# # first let's eliminate non-english songs by their names\n",
    "# before = df.shape[0]\n",
    "# for row_id in range(100):\n",
    "#     text = df.loc[row_id]['song']\n",
    "#     diff = eng_ratio(text)\n",
    "#     if diff >= 0.5:\n",
    "#         df = df[df.index != row_id]\n",
    "# after = df.shape[0]\n",
    "# rem = before - after\n",
    "# print('%s have been removed.' %rem)\n",
    "# print('%s songs remain in the dataset.' %after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English Filter Ver.2\n",
    "This is another approach, which uses a package from https://github.com/saffsd/langid.py. This package can detect language in a fairly quicker way. But still, 260k records takes around 50 mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23693 have been removed.\n",
      "242863 songs remain in the dataset.\n"
     ]
    }
   ],
   "source": [
    "# # package from https://github.com/saffsd/langid.py\n",
    "# import langid\n",
    "\n",
    "# before = df.shape[0]\n",
    "# for row in df.index:\n",
    "#     lang = langid.classify(df.loc[row]['lyrics'])[0]\n",
    "#     if lang != 'en':\n",
    "#         df = df[df.index != row]\n",
    "# after = df.shape[0]\n",
    "\n",
    "# rem = before - after\n",
    "# print('%s have been removed.' %rem)\n",
    "# print('%s songs remain in the dataset.' %after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save english songs to a new csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.to_csv('lyrics_new.csv',index_label='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "### Re-read csv file as df\n",
    "Now only English songs exists in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rock             102619\n",
       "Pop               34919\n",
       "Hip-Hop           23042\n",
       "Metal             22249\n",
       "Not Available     18654\n",
       "Country           14307\n",
       "Jazz               7498\n",
       "Electronic         7374\n",
       "Other              3951\n",
       "R&B                3362\n",
       "Indie              3010\n",
       "Folk               1878\n",
       "Name: genre, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./lyrics_new.csv\", encoding=\"utf-8\").drop('index.1', axis=1)\n",
    "df.genre.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling  df --> df_sample\n",
    "300k records easily run out of memory. So I tried to resample the dataset and choose equal size of each genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataframe: 21600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Country          1800\n",
       "Indie            1800\n",
       "Hip-Hop          1800\n",
       "Rock             1800\n",
       "Folk             1800\n",
       "Electronic       1800\n",
       "Pop              1800\n",
       "Other            1800\n",
       "Metal            1800\n",
       "Jazz             1800\n",
       "Not Available    1800\n",
       "R&B              1800\n",
       "Name: genre, dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = df.groupby('genre')\n",
    "df_sample = grouped.apply(lambda x: x.sample(n=1800, random_state=7))\n",
    "\n",
    "print(\"Size of dataframe: {}\".format(df_sample.shape[0]))\n",
    "      \n",
    "df_sample.genre.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>song</th>\n",
       "      <th>year</th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>104901</td>\n",
       "      <td>it-s-great-to-be-single-again</td>\n",
       "      <td>2007</td>\n",
       "      <td>david-allan-coe</td>\n",
       "      <td>Country</td>\n",
       "      <td>No more dirty dishes in the sink when I come h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>216767</td>\n",
       "      <td>how-can-you-buy-killarney</td>\n",
       "      <td>2007</td>\n",
       "      <td>charlie-landsborough</td>\n",
       "      <td>Country</td>\n",
       "      <td>An American landed on Erin's green isle\\nHe ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>126582</td>\n",
       "      <td>sawing-on-the-strings</td>\n",
       "      <td>2007</td>\n",
       "      <td>alison-krauss</td>\n",
       "      <td>Country</td>\n",
       "      <td>Way back in the mountains\\nWay back in the hil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>129927</td>\n",
       "      <td>i-don-t-believe-you-ve-met-my-baby</td>\n",
       "      <td>2006</td>\n",
       "      <td>dolly-parton</td>\n",
       "      <td>Country</td>\n",
       "      <td>Last night, my tears they were fallin'\\nI went...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>218507</td>\n",
       "      <td>please-don-t-hurry-your-heart</td>\n",
       "      <td>2008</td>\n",
       "      <td>caitlin-cary</td>\n",
       "      <td>Country</td>\n",
       "      <td>Oh, when you're leaving for the hundredth time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>80092</td>\n",
       "      <td>new-dug-grave</td>\n",
       "      <td>2007</td>\n",
       "      <td>gillian-welch</td>\n",
       "      <td>Country</td>\n",
       "      <td>I left home when I was twenty\\nJust to see wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>310224</td>\n",
       "      <td>no-memories-hangin-round</td>\n",
       "      <td>2015</td>\n",
       "      <td>bobby-bare</td>\n",
       "      <td>Country</td>\n",
       "      <td>You don't want no more heartaches\\nAnd I don't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>68325</td>\n",
       "      <td>that-s-how-much-i-love-you</td>\n",
       "      <td>2014</td>\n",
       "      <td>eddy-arnold</td>\n",
       "      <td>Country</td>\n",
       "      <td>Well if I had a nickel I know what I would do\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>215612</td>\n",
       "      <td>i-m-fine-either-way</td>\n",
       "      <td>2007</td>\n",
       "      <td>bobby-pinson</td>\n",
       "      <td>Country</td>\n",
       "      <td>Come on\\nMouth full of blood one eye swoll shu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>191238</td>\n",
       "      <td>c-mon</td>\n",
       "      <td>2014</td>\n",
       "      <td>amber-hayes</td>\n",
       "      <td>Country</td>\n",
       "      <td>Hey, hey I'm lookin' at you\\nBoy I gotta tell ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                                song  year                artist  \\\n",
       "0  104901       it-s-great-to-be-single-again  2007       david-allan-coe   \n",
       "1  216767           how-can-you-buy-killarney  2007  charlie-landsborough   \n",
       "2  126582               sawing-on-the-strings  2007         alison-krauss   \n",
       "3  129927  i-don-t-believe-you-ve-met-my-baby  2006          dolly-parton   \n",
       "4  218507       please-don-t-hurry-your-heart  2008          caitlin-cary   \n",
       "5   80092                       new-dug-grave  2007         gillian-welch   \n",
       "6  310224            no-memories-hangin-round  2015            bobby-bare   \n",
       "7   68325          that-s-how-much-i-love-you  2014           eddy-arnold   \n",
       "8  215612                 i-m-fine-either-way  2007          bobby-pinson   \n",
       "9  191238                               c-mon  2014           amber-hayes   \n",
       "\n",
       "     genre                                             lyrics  \n",
       "0  Country  No more dirty dishes in the sink when I come h...  \n",
       "1  Country  An American landed on Erin's green isle\\nHe ga...  \n",
       "2  Country  Way back in the mountains\\nWay back in the hil...  \n",
       "3  Country  Last night, my tears they were fallin'\\nI went...  \n",
       "4  Country  Oh, when you're leaving for the hundredth time...  \n",
       "5  Country  I left home when I was twenty\\nJust to see wha...  \n",
       "6  Country  You don't want no more heartaches\\nAnd I don't...  \n",
       "7  Country  Well if I had a nickel I know what I would do\\...  \n",
       "8  Country  Come on\\nMouth full of blood one eye swoll shu...  \n",
       "9  Country  Hey, hey I'm lookin' at you\\nBoy I gotta tell ...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reset index means remove index (and change index to a column if not drop)\n",
    "df_sample.reset_index(drop=True, inplace=True)\n",
    "df_sample.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the lyrics' quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instrumental\n",
      "This track is an instrumental and has no lyrics.\n",
      "guitars and cadilacs\n",
      "hillbilly music\n",
      "only thing that keeps me hanging on\n",
      "instrumental\n",
      "INSTRUMENTAL\n",
      "\n",
      "Num of lyrics with length less than 100 in first 1000: 5\n"
     ]
    }
   ],
   "source": [
    "# check lyrics with length less than 100\n",
    "less_than_100 = 0\n",
    "for row in df_sample.index[:1000]:\n",
    "    if len(df_sample.loc[row]['lyrics'])<=100:\n",
    "        print(df_sample.loc[row]['lyrics'])\n",
    "        less_than_100 += 1\n",
    "print(\"\\nNum of lyrics with length less than 100 in first 1000: {}\".format(less_than_100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like lots of songs don't have meaningful lyrics(instrumental music, or something wrong happened when crawling).\n",
    "\n",
    "So we just drop all song records with less than 100 lyric length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_sample --> df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting records with lyric length < 100\n",
      "Before: 21600\n",
      "After : 20954\n",
      "Deleted: 646\n"
     ]
    }
   ],
   "source": [
    "print(\"Deleting records with lyric length < 100\")\n",
    "\n",
    "len_before = df_sample.shape[0]\n",
    "\n",
    "df_clean = df_sample.copy()\n",
    "\n",
    "for row in df_clean.index:\n",
    "    if len(df_clean.loc[row]['lyrics']) <= 100:\n",
    "        df_clean.drop(row, inplace=True)\n",
    "\n",
    "len_after = df_clean.shape[0]\n",
    "\n",
    "print(\"Before: {}\\nAfter : {}\\nDeleted: {}\".format(len_before, len_after, len_before-len_after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country          1791\n",
       "R&B              1788\n",
       "Other            1783\n",
       "Pop              1779\n",
       "Hip-Hop          1771\n",
       "Indie            1768\n",
       "Jazz             1758\n",
       "Rock             1756\n",
       "Metal            1723\n",
       "Not Available    1694\n",
       "Electronic       1686\n",
       "Folk             1657\n",
       "Name: genre, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.genre.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### transfer lyrics to list  \n",
    "### df_clean --> x & y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of x: 20954\n",
      "Size of y: 20954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"An American landed on Erin's green isle\\nHe gazed on killarny with a rapturous smile\\nHow can I buy it he said to the guy\\nI'll tell you how with a smile he replied\\nHow can you buy all the stars in the sky\\nHow can you buy two Blue Irish eyes\\nWhen you can purchase a fine mothers heart\\nThen you can buy killarny\\nNature restore on her guilt's with a smile\\nMe and Rose the shamrock and the barley\\nWhen you can buy all those wonderful things\\nThen you can buy killarny\\nOver in Killarny, Many years ago,\\nthere's a song my mother sang to me\\nin a voice so sweet and low.\\nJust a simple Irish ditty,\\nIn her sweet ould fashion way,\\nAnd I'd give the world if I could hear\\nThat song of hers today.\\nToo-ra-loo-ra-loo-ral,\\nToo-ra-loo-ra-li,\\nToo-ra-loo-ra-loo-ral,\\nHush, now don't you cry!\\nToo-ra-loo-ra-loo-ral,\\nToo-ra-loo-ra-li,\\nToo-ra-loo-ra-loo-ral,\\nThat's an Irish lullaby.\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df_clean['lyrics'].values\n",
    "y = df_clean['genre'].values\n",
    "print('Size of x: {}\\nSize of y: {}'.format(x.size, y.size))\n",
    "\n",
    "x = x.tolist()\n",
    "\n",
    "x[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### removing punctuation and \\n\n",
    "\n",
    "reference: https://stackoverflow.com/questions/13970203/how-to-count-average-sentence-length-in-words-from-a-text-file-contains-100-se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def count_sentence_len(lyric):\n",
    "#     \"\"\"count average sentence len for a lyric\"\"\"\n",
    "#     sents_list = lyric.split('\\n')\n",
    "#     avg_len = sum(len(x.split()) for x in sents_list) / len(sents_list)\n",
    "#     return avg_len\n",
    "\n",
    "# sentence_length_avg = []\n",
    "\n",
    "x_clean = []\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "for l in x:\n",
    "    l = l.translate(translator)\n",
    "#     sentence_len = count_sentence_len(l)\n",
    "#     sentence_length_avg.append(sentence_len)\n",
    "    l = l.replace('\\n', ' ')\n",
    "    \n",
    "    x_clean.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verse 1 Another night stuck at home all alone Please help me out I want to roam PreChorus Cause living life through a window aint no life at all trapped in tiny room thats way to small its way to small Verse 2 I think about you from 9 to 3 Thats when theres nothing on TV PreChorus 1X Chorus I need someonewho cares yeah theyll be thereyeah theyll be there to help me outwithout a doubt yeah theyll be thereyeah theyll be there Verse 3 I used to be an ok guy now im a home fly So much to tell my rooms my cell PreChorus 1X Chorus1X Bridge Cause its so lame when no one knows your name and its so sad that my life is so bad Chorus 1X\n",
      "=============================\n",
      "Youll be on my side Somethings burning my way I would kill for you No way to trap your game I was so faithful I wanna tear your lies My fevers rising torn inside This is the noise I break the future Emotional I nearly died Ill strip away Im tired of violence This is the noise I break the future I can predict what drives you on Floating out to wonderland Be back to frame you What you get erasure Troubles on the line We run on and on Fingers and triggers Catch you on your knees Still I remain calm My precious you ran into luck\n",
      "=============================\n",
      "Rad attitude and my nose is a running I like you lots but you think that Im a dummy Can I pick you up with me and my buddies and chill Well hammer down in my Plymouth Barracuda Huffin and a puffin on that BC Buddah Dont worry bout it sugar you got nothing to lose Come along and take a ride with me Ill make some space in my dirty back seat Ill crank the credence push the pedal to the metal round town Well laugh about this tomorrow cos times like this I hope will follow Rad attitude and my nose is a running I like you lots but you think that Im a dummy Can I pick you up with me and my buddies and chill Well hammer down in my Plymouth Barracuda Huffin and a puffin on that BC Buddah Dont worry bout it sugar you got nothing to lose Come along and take a ride with me Ill make some space in my dirty back seat Ill crank the credence push the pedal to the metal round town Well laugh about this tomorrow cos times like this I hope will follow me i hope they follow me i hope they follow me oh oh i hope they follow me Come along and take a ride with me Ill make some space in my dirty back seat Ill crank the credence push the pedal to the metal round town Well laugh about this tomorrow cos times like this I hope\n",
      "=============================\n",
      "I need your sweet sweet inspiration I need you here on my mind every hour of the day Without your sweet sweet inspiration The lonely hours of the day just dont go my way A woman in love needs sweet inspiration And honey thats all I ask You know thats all I ask from you I gotta have your sweet inspiration yeah You know there just aint no telling What a satisfied woman might do When you call Baby baby Is such a sweet inspiration And when you call me Darling darling Sets my heart askating And if Im out in the rain baby And in a bad situation You know I just reach back in my mind And there I find your sweet sweet inspiration Inspiration yeah oh what a power And I got the power every hour of the day I need your sweet sweet sweet sweet inspiration yeah To go on living to keep on giving this way I need your sweet sweet inspiration And I got the power every hour of the day I need your sweet sweet inspiration To go on living to keep on giving this way\n",
      "=============================\n",
      "There was a fair young lady so lately Ive been told She lived with her uncle the cause of all her woes Her uncle had a ploughboy which Mollie liked quite well And in her uncles garden their tender love did tell So early one morning this old man he arose And at Mollies room door he hastened on his clothes Saying  Arise you handsome female and married you shall be For the squire is awaiting on the banks of sweet Dundee A fig to all your squires to lord and Jews likewise For William pears like diamonds aglittering in my eyes You never shall have Willie nor happy shall you be For I mean to banish Willie from the banks of sweet Dundee The first crowd came on Willie when he was all alone He fought full hard for his liberty but there were eight to one Pray kill me now says Willie Pray kill me now says he For Id rather die for Mollie on the banks of sweet Dundee As Mollie was walking lamenting for her love She meets the wealthy squire all in her uncles grove Stand off stand off says Mollie Stand off you man says she For Id rather die for Willie on the banks of sweet Dundee He threw his arms around her and crushed her to the ground There she spied two pistols and a sword beneath his morninggown The pistols she slipped slyly and the sword she used free She shot and killed the squire on the banks of sweet Dundee Her uncle overheard them come hastening to the grove Saying Youve killed the wealthy squire prepare for your deathblow Stand off stand off says Mollie Stand off you man says she So the trigger drew and her uncle slew on the banks of sweet Dundee The doctor being sent for he knew that they were killed Also there came a lawyer to write the old mans will He willed his gold to Mollie because she fought so free Then closed his eyes to write no more on the banks of sweet Dundee\n",
      "=============================\n"
     ]
    }
   ],
   "source": [
    "# randomly print 5 lyrics\n",
    "import random\n",
    "for i in random.sample(range(len(x_clean)), 5):\n",
    "    print(x_clean[i])\n",
    "    print(\"=============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20954\n"
     ]
    }
   ],
   "source": [
    "print(len(x_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Removing stop words\n",
    "nltk package has a build in library of stop words. Here I build my own stop-words dictionary basing on sklearn buildin stop word dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.9 s, sys: 136 ms, total: 20 s\n",
      "Wall time: 20.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_clean = [x.lower() for x in x_clean]\n",
    "\n",
    "x_clean_new = []\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "stop_words = list(ENGLISH_STOP_WORDS)\n",
    "stop_words = stop_words + ['will', 'got', 'ill', 'im', 'let']\n",
    "\n",
    "for text in x_clean:\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    x_clean_new.append(text)\n",
    "    \n",
    "x_clean = x_clean_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Bag-of-words representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I used a english dictionary from https://github.com/eclarson/MachineLearningNotebooks/tree/master/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79340\n"
     ]
    }
   ],
   "source": [
    "with open('./ospd.txt', encoding='utf-8', errors='ignore') as f1:\n",
    "    vocab1 = f1.read().split(\"\\n\")\n",
    "\n",
    "print(len(vocab1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of bag words: (20954, 79340)\n",
      "Length of Vocabulary: 79340\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "\n",
    "# CounterVectorizer can automatically change words into lower case\n",
    "cv = CountVectorizer(stop_words='english',\n",
    "                    encoding='utf-8',\n",
    "                    lowercase=True,\n",
    "                    vocabulary=vocab1)\n",
    "\n",
    "bag_words = cv.fit_transform(x_clean)\n",
    "\n",
    "print('Shape of bag words: {}'.format(bag_words.shape))\n",
    "print(\"Length of Vocabulary: {}\".format(len(cv.vocabulary_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's createe a pandas dataframe containing bag-of-words(bow) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aah</th>\n",
       "      <th>aahed</th>\n",
       "      <th>aahing</th>\n",
       "      <th>aahs</th>\n",
       "      <th>aal</th>\n",
       "      <th>aalii</th>\n",
       "      <th>aaliis</th>\n",
       "      <th>aals</th>\n",
       "      <th>aardvark</th>\n",
       "      <th>aardwolf</th>\n",
       "      <th>aargh</th>\n",
       "      <th>aarrgh</th>\n",
       "      <th>aarrghh</th>\n",
       "      <th>aas</th>\n",
       "      <th>aasvogel</th>\n",
       "      <th>aba</th>\n",
       "      <th>abaca</th>\n",
       "      <th>abacas</th>\n",
       "      <th>abaci</th>\n",
       "      <th>aback</th>\n",
       "      <th>abacus</th>\n",
       "      <th>abacuses</th>\n",
       "      <th>abaft</th>\n",
       "      <th>abaka</th>\n",
       "      <th>abakas</th>\n",
       "      <th>abalone</th>\n",
       "      <th>abalones</th>\n",
       "      <th>abamp</th>\n",
       "      <th>abampere</th>\n",
       "      <th>...</th>\n",
       "      <th>zygoid</th>\n",
       "      <th>zygoma</th>\n",
       "      <th>zygomas</th>\n",
       "      <th>zygomata</th>\n",
       "      <th>zygose</th>\n",
       "      <th>zygoses</th>\n",
       "      <th>zygosis</th>\n",
       "      <th>zygosity</th>\n",
       "      <th>zygote</th>\n",
       "      <th>zygotene</th>\n",
       "      <th>zygotes</th>\n",
       "      <th>zygotic</th>\n",
       "      <th>zymase</th>\n",
       "      <th>zymases</th>\n",
       "      <th>zyme</th>\n",
       "      <th>zymes</th>\n",
       "      <th>zymogen</th>\n",
       "      <th>zymogene</th>\n",
       "      <th>zymogens</th>\n",
       "      <th>zymogram</th>\n",
       "      <th>zymology</th>\n",
       "      <th>zymosan</th>\n",
       "      <th>zymosans</th>\n",
       "      <th>zymoses</th>\n",
       "      <th>zymosis</th>\n",
       "      <th>zymotic</th>\n",
       "      <th>zymurgy</th>\n",
       "      <th>zyzzyva</th>\n",
       "      <th>zyzzyvas</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 79340 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  aah  aahed  aahing  aahs  aal  aalii  aaliis  aals  aardvark  aardwolf  \\\n",
       "0   0    0      0       0     0    0      0       0     0         0         0   \n",
       "1   0    0      0       0     0    0      0       0     0         0         0   \n",
       "2   0    0      0       0     0    0      0       0     0         0         0   \n",
       "3   0    0      0       0     0    0      0       0     0         0         0   \n",
       "4   0    0      0       0     0    0      0       0     0         0         0   \n",
       "\n",
       "   aargh  aarrgh  aarrghh  aas  aasvogel  aba  abaca  abacas  abaci  aback  \\\n",
       "0      0       0        0    0         0    0      0       0      0      0   \n",
       "1      0       0        0    0         0    0      0       0      0      0   \n",
       "2      0       0        0    0         0    0      0       0      0      0   \n",
       "3      0       0        0    0         0    0      0       0      0      0   \n",
       "4      0       0        0    0         0    0      0       0      0      0   \n",
       "\n",
       "   abacus  abacuses  abaft  abaka  abakas  abalone  abalones  abamp  abampere  \\\n",
       "0       0         0      0      0       0        0         0      0         0   \n",
       "1       0         0      0      0       0        0         0      0         0   \n",
       "2       0         0      0      0       0        0         0      0         0   \n",
       "3       0         0      0      0       0        0         0      0         0   \n",
       "4       0         0      0      0       0        0         0      0         0   \n",
       "\n",
       "  ...  zygoid  zygoma  zygomas  zygomata  zygose  zygoses  zygosis  zygosity  \\\n",
       "0 ...       0       0        0         0       0        0        0         0   \n",
       "1 ...       0       0        0         0       0        0        0         0   \n",
       "2 ...       0       0        0         0       0        0        0         0   \n",
       "3 ...       0       0        0         0       0        0        0         0   \n",
       "4 ...       0       0        0         0       0        0        0         0   \n",
       "\n",
       "   zygote  zygotene  zygotes  zygotic  zymase  zymases  zyme  zymes  zymogen  \\\n",
       "0       0         0        0        0       0        0     0      0        0   \n",
       "1       0         0        0        0       0        0     0      0        0   \n",
       "2       0         0        0        0       0        0     0      0        0   \n",
       "3       0         0        0        0       0        0     0      0        0   \n",
       "4       0         0        0        0       0        0     0      0        0   \n",
       "\n",
       "   zymogene  zymogens  zymogram  zymology  zymosan  zymosans  zymoses  \\\n",
       "0         0         0         0         0        0         0        0   \n",
       "1         0         0         0         0        0         0        0   \n",
       "2         0         0         0         0        0         0        0   \n",
       "3         0         0         0         0        0         0        0   \n",
       "4         0         0         0         0        0         0        0   \n",
       "\n",
       "   zymosis  zymotic  zymurgy  zyzzyva  zyzzyvas     \n",
       "0        0        0        0        0         0  0  \n",
       "1        0        0        0        0         0  0  \n",
       "2        0        0        0        0         0  0  \n",
       "3        0        0        0        0         0  0  \n",
       "4        0        0        0        0         0  0  \n",
       "\n",
       "[5 rows x 79340 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bow = pd.DataFrame(data=bag_words.toarray(),columns=cv.get_feature_names())\n",
    "df_bow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.1 s, sys: 6.87 s, total: 51 s\n",
      "Wall time: 53.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_freq = df_bow.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "love      31574\n",
       "like      27405\n",
       "know      26740\n",
       "just      24935\n",
       "oh        19478\n",
       "time      15466\n",
       "baby      13818\n",
       "want      13161\n",
       "come      12529\n",
       "cause     12482\n",
       "way       12077\n",
       "say       11882\n",
       "make      11646\n",
       "yeah      11150\n",
       "life       9547\n",
       "heart      9446\n",
       "right      9295\n",
       "feel       9116\n",
       "away       9067\n",
       "need       8847\n",
       "day        8622\n",
       "night      8189\n",
       "tell       8186\n",
       "man        8107\n",
       "girl       7367\n",
       "world      7097\n",
       "good       6845\n",
       "think      6827\n",
       "theres     6812\n",
       "little     6764\n",
       "dtype: int64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Tf-idf representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of bag words: (20954, 79340)\n",
      "Length of Vocabulary: 79340\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer(stop_words='english',\n",
    "                             encoding='utf-8',\n",
    "                             lowercase=True,\n",
    "                             vocabulary=vocab1)\n",
    "\n",
    "tfidf_mat = tfidf_vect.fit_transform(x_clean)\n",
    "\n",
    "print('Shape of bag words: {}'.format(tfidf_mat.shape))\n",
    "print(\"Length of Vocabulary: {}\".format(len(tfidf_vect.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aah</th>\n",
       "      <th>aahed</th>\n",
       "      <th>aahing</th>\n",
       "      <th>aahs</th>\n",
       "      <th>aal</th>\n",
       "      <th>aalii</th>\n",
       "      <th>aaliis</th>\n",
       "      <th>aals</th>\n",
       "      <th>aardvark</th>\n",
       "      <th>aardwolf</th>\n",
       "      <th>aargh</th>\n",
       "      <th>aarrgh</th>\n",
       "      <th>aarrghh</th>\n",
       "      <th>aas</th>\n",
       "      <th>aasvogel</th>\n",
       "      <th>aba</th>\n",
       "      <th>abaca</th>\n",
       "      <th>abacas</th>\n",
       "      <th>abaci</th>\n",
       "      <th>aback</th>\n",
       "      <th>abacus</th>\n",
       "      <th>abacuses</th>\n",
       "      <th>abaft</th>\n",
       "      <th>abaka</th>\n",
       "      <th>abakas</th>\n",
       "      <th>abalone</th>\n",
       "      <th>abalones</th>\n",
       "      <th>abamp</th>\n",
       "      <th>abampere</th>\n",
       "      <th>...</th>\n",
       "      <th>zygoid</th>\n",
       "      <th>zygoma</th>\n",
       "      <th>zygomas</th>\n",
       "      <th>zygomata</th>\n",
       "      <th>zygose</th>\n",
       "      <th>zygoses</th>\n",
       "      <th>zygosis</th>\n",
       "      <th>zygosity</th>\n",
       "      <th>zygote</th>\n",
       "      <th>zygotene</th>\n",
       "      <th>zygotes</th>\n",
       "      <th>zygotic</th>\n",
       "      <th>zymase</th>\n",
       "      <th>zymases</th>\n",
       "      <th>zyme</th>\n",
       "      <th>zymes</th>\n",
       "      <th>zymogen</th>\n",
       "      <th>zymogene</th>\n",
       "      <th>zymogens</th>\n",
       "      <th>zymogram</th>\n",
       "      <th>zymology</th>\n",
       "      <th>zymosan</th>\n",
       "      <th>zymosans</th>\n",
       "      <th>zymoses</th>\n",
       "      <th>zymosis</th>\n",
       "      <th>zymotic</th>\n",
       "      <th>zymurgy</th>\n",
       "      <th>zyzzyva</th>\n",
       "      <th>zyzzyvas</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 79340 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aa  aah  aahed  aahing  aahs  aal  aalii  aaliis  aals  aardvark  \\\n",
       "0  0.0  0.0    0.0     0.0   0.0  0.0    0.0     0.0   0.0       0.0   \n",
       "1  0.0  0.0    0.0     0.0   0.0  0.0    0.0     0.0   0.0       0.0   \n",
       "2  0.0  0.0    0.0     0.0   0.0  0.0    0.0     0.0   0.0       0.0   \n",
       "3  0.0  0.0    0.0     0.0   0.0  0.0    0.0     0.0   0.0       0.0   \n",
       "4  0.0  0.0    0.0     0.0   0.0  0.0    0.0     0.0   0.0       0.0   \n",
       "\n",
       "   aardwolf  aargh  aarrgh  aarrghh  aas  aasvogel  aba  abaca  abacas  abaci  \\\n",
       "0       0.0    0.0     0.0      0.0  0.0       0.0  0.0    0.0     0.0    0.0   \n",
       "1       0.0    0.0     0.0      0.0  0.0       0.0  0.0    0.0     0.0    0.0   \n",
       "2       0.0    0.0     0.0      0.0  0.0       0.0  0.0    0.0     0.0    0.0   \n",
       "3       0.0    0.0     0.0      0.0  0.0       0.0  0.0    0.0     0.0    0.0   \n",
       "4       0.0    0.0     0.0      0.0  0.0       0.0  0.0    0.0     0.0    0.0   \n",
       "\n",
       "   aback  abacus  abacuses  abaft  abaka  abakas  abalone  abalones  abamp  \\\n",
       "0    0.0     0.0       0.0    0.0    0.0     0.0      0.0       0.0    0.0   \n",
       "1    0.0     0.0       0.0    0.0    0.0     0.0      0.0       0.0    0.0   \n",
       "2    0.0     0.0       0.0    0.0    0.0     0.0      0.0       0.0    0.0   \n",
       "3    0.0     0.0       0.0    0.0    0.0     0.0      0.0       0.0    0.0   \n",
       "4    0.0     0.0       0.0    0.0    0.0     0.0      0.0       0.0    0.0   \n",
       "\n",
       "   abampere ...   zygoid  zygoma  zygomas  zygomata  zygose  zygoses  zygosis  \\\n",
       "0       0.0 ...      0.0     0.0      0.0       0.0     0.0      0.0      0.0   \n",
       "1       0.0 ...      0.0     0.0      0.0       0.0     0.0      0.0      0.0   \n",
       "2       0.0 ...      0.0     0.0      0.0       0.0     0.0      0.0      0.0   \n",
       "3       0.0 ...      0.0     0.0      0.0       0.0     0.0      0.0      0.0   \n",
       "4       0.0 ...      0.0     0.0      0.0       0.0     0.0      0.0      0.0   \n",
       "\n",
       "   zygosity  zygote  zygotene  zygotes  zygotic  zymase  zymases  zyme  zymes  \\\n",
       "0       0.0     0.0       0.0      0.0      0.0     0.0      0.0   0.0    0.0   \n",
       "1       0.0     0.0       0.0      0.0      0.0     0.0      0.0   0.0    0.0   \n",
       "2       0.0     0.0       0.0      0.0      0.0     0.0      0.0   0.0    0.0   \n",
       "3       0.0     0.0       0.0      0.0      0.0     0.0      0.0   0.0    0.0   \n",
       "4       0.0     0.0       0.0      0.0      0.0     0.0      0.0   0.0    0.0   \n",
       "\n",
       "   zymogen  zymogene  zymogens  zymogram  zymology  zymosan  zymosans  \\\n",
       "0      0.0       0.0       0.0       0.0       0.0      0.0       0.0   \n",
       "1      0.0       0.0       0.0       0.0       0.0      0.0       0.0   \n",
       "2      0.0       0.0       0.0       0.0       0.0      0.0       0.0   \n",
       "3      0.0       0.0       0.0       0.0       0.0      0.0       0.0   \n",
       "4      0.0       0.0       0.0       0.0       0.0      0.0       0.0   \n",
       "\n",
       "   zymoses  zymosis  zymotic  zymurgy  zyzzyva  zyzzyvas       \n",
       "0      0.0      0.0      0.0      0.0      0.0       0.0  0.0  \n",
       "1      0.0      0.0      0.0      0.0      0.0       0.0  0.0  \n",
       "2      0.0      0.0      0.0      0.0      0.0       0.0  0.0  \n",
       "3      0.0      0.0      0.0      0.0      0.0       0.0  0.0  \n",
       "4      0.0      0.0      0.0      0.0      0.0       0.0  0.0  \n",
       "\n",
       "[5 rows x 79340 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf = pd.DataFrame(data=tfidf_mat.toarray(),columns=tfidf_vect.get_feature_names())\n",
    "df_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "word_score = df_tfidf.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_score[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate the corelation matrix, where number in each position (i,j) represents the correlation between song i and song j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corr = (tfidf_mat * tfidf_mat.T).A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Visualization\n",
    "### 3.1 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "freq = pd.DataFrame(word_freq, columns = ['frequency'])\n",
    "fig = freq[:20].plot(kind = 'barh', figsize=(9,8), fontsize=18)\n",
    "# plt.legend('number of occurrences', loc = 'upper right')\n",
    "\n",
    "\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('words frequencies', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in this histogram, the top frequent words are \"love\", \"know\", \"like\" and so on. Among these top 20 frequent words listed in the histogram, the frequency of the top 4 words (love, know, like, just) is almost trible of the last 3 words, i.e. there's a considerable difference between the frequency of different words. One more thing we notice is that, there is a interjection in the list, \"oh\", and it is the top 6 frequent word. We didn't even notice artists used so many \"oh\" in the lyrics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = pd.DataFrame(word_score, columns = ['Score'])\n",
    "ax = score[:20].plot(kind = 'barh', figsize=(9,8), fontsize=18)\n",
    "plt.legend('score', loc = 'lower right', fontsize=15)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('tf-idf score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To figure out the most frequency word for each genre, TF-IDF may be more appropriate (given that TF-IDF reflects how important words to the document). \n",
    "From the plot above, we can see that the top frequent words are totally different from those words listed according to term frequency. \n",
    "\n",
    "And we can see that there are some words, like \"al\", \"bo\", \"dor\", \"la\", have high TF-IDF score. This may due to the phynominon that these words only exist in some documents (songs), makes them so \"special\" and are highlighted as important words for documents.\n",
    "\n",
    "TF-IDF analysis for each genre is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# code example from https://www.kaggle.com/carrie1/drug-of-choice-by-genre-using-song-lyrics\n",
    "df_clean['word_count'] = df_clean['lyrics'].str.split().str.len()\n",
    "df_clean.info()\n",
    "f, ax = plt.subplots(figsize=(10, 9))\n",
    "sns.violinplot(x = df_clean.word_count)\n",
    "plt.xlim(-100, 1000)\n",
    "plt.title('Word count distribution', fontsize=26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The violinplot plot the distribution of all the songs according to number of words of lyrics.\n",
    "\n",
    "The figure shows that most of songs have lyric length form 100 to 300 words. The lyric length median locates around 200. And only a small part of songs' lyric length longer than 400.\n",
    "\n",
    "This make sense for the real lyrics length. After all, people may get tired of songs with too many lyrics and are more unlikely to fall in love with the songs only have a few words. \n",
    "\n",
    "Above plot is for all lyrics, without classifying by genre. We still cannot get the desired feature for each genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 9))\n",
    "sns.boxplot(x = \"genre\", y = \"word_count\", data = df_clean, palette = \"Set1\")\n",
    "plt.ylim(1,2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To figure out the lyric length feature for each genre. We group the data by genre and get box plot for each genre. \n",
    "\n",
    "According to the plot, medians of most box are under 250 (around 200). Only the median for Hip-Hop is around 500, more than double length than the others. For the maximum, Electronic, Rock, Hip-Hop have the top 3 longest lyrics. And there's no big difference for the minimum for all the genres.\n",
    "\n",
    "In general, the top 5 longest lyrics genres(named) are Hip-Hop, Pop, R&B, Electronic, Indie. The last 3 genres(named) are Jazz, Metal, Country. It seems that the genres with up tempo are more likely to have longer lyrics, and vice-versa. But we still need to pay attention to some exception. Metal songs with up tempo, however, mostly they have shorter lyrics than the other up-tempo songs. Thus, the length of lyrics can be a reference for genre classification but should not be the decision metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distribution across time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mpl.rc(\"figure\", figsize=(12,12))\n",
    "sns.violinplot(x='genre', y='year', data=df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the distribution is biased with extreme values. So let's check outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean[df_clean['year'] <= 2000].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop songs before 2000 and plot again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row in df_clean[df_clean['year'] <= 2000].index:\n",
    "    df_clean.drop(row, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mpl.rc(\"figure\", figsize=(15, 25))\n",
    "sns.violinplot(x='year', y='genre', data=df_clean, inner=\"quartile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the distributions are quite different. Country, Metal, Pop, R&B and Rock have a more centrilized distribution, mostly created during 2005~2010. Other genres have a quite streched distribution. Other songs(song's not labled with a genre) are mostly composed after 2012, propably because new songs don't have labels yet. \n",
    "\n",
    "Several geners had a big-bang around 2006~2009. We are wondering if this distribution was due to reality or just crawlling problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_artist = df.artist.value_counts().head(8).index.tolist()\n",
    "\n",
    "# df_clean['artist'].isin(top_artist)\n",
    "# df_clean.loc[df_clean['artist'] in]\n",
    "\n",
    "df_top_artist = df_clean.loc[df_clean['artist'].isin(top_artist), :]\n",
    "df_top_artist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_top_artist.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mpl.rc(\"figure\", figsize=(25, 15))\n",
    "sns.violinplot(x='artist', y='year', data=df_top_artist, inner=\"quartile\")\n",
    "sns.set(font_scale=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the top 8 artists, we plot this figure to explore their high-yield years.\n",
    "For artist eddy-arnold, dolly-parton, eminem, barba-streisan and bee-gees, their most songs were composed during 2005~2010. And for the cris-crown and bob dylan, it seems they kept creating for a long time. However, bob-dylan's works are sort of \"ahead of time\". It may due to the mis-input of the information.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df_bow.shape)\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### length of songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_bow['length'] = df_bow.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create two new columns: \n",
    "# @ length: length of documents basing on bag-of-word model\n",
    "# @ genre: genre of the record\n",
    "\n",
    "df_bow['genre'] = pd.Series(y).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mpl.rc(\"figure\", figsize=(25, 15))\n",
    "sns.violinplot(x='length', y='genre', data=df_bow, inner=\"quartile\")\n",
    "sns.set(font_scale=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is another way to calculate lyrics' length basing on word bags. The violin plot for the lyric length among each genre plot corresponding to the box-plot above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to check the top 10 frequent words of each genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "genre_count = df_bow.groupby('genre').sum()\n",
    "genre_count.drop('length', axis=1, inplace=True)\n",
    "genre_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genre_count_new = genre_count.transpose()\n",
    "genre_list = df_clean.genre.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for genre in genre_list:\n",
    "    t = genre_count_new.nlargest(10, genre, keep='first')[genre]\n",
    "    \n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "    fig.suptitle(genre, fontsize=20)\n",
    "    plt.xticks(rotation='vertical')\n",
    "    sns.barplot(t.values, t.index, alpha=0.8)\n",
    "sns.set(font_scale=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above histogram, we list some top frequent words for each genre. For different genres, they have top-10 frequent words in common and. And these information can be visualized in word cloud figures in Part 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From those histograms, it is pretty straightforward that 'love' is almost every types of music cared about. And also other words they share in common, which are 'know','time', 'oh' etc. And also many of those words are verbs.\n",
    "It looks like hip-hop music has a quite differnet set of frequent words, distinctive from other genres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word Cloud\n",
    "Now it is 'wordcloud' time.\n",
    "Word cloud is a visual representation of text data, and it is a very efficient way to represent word frequencies.\n",
    "\n",
    "First let's try to draw the overall wordcloud basing on term frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "all_lyrics = ''\n",
    "for lyric in x_clean:\n",
    "    all_lyrics += (' '+lyric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# code example from https://amueller.github.io/word_cloud/index.html\n",
    "wordcloud = WordCloud(max_font_size=60).generate(all_lyrics)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the most frequently used word is 'love' over all, then comes with 'got',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_freq[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the word cloud describes word frequency in a visuable way.\n",
    "\n",
    "Let's try plot word clouds in different genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = {'genre': y.tolist(), \"lyric\": x_clean}\n",
    "df_plot = pd.DataFrame(d)\n",
    "df_plot.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's separate those lyrics into different genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a dictionary and store all lyrics basing on their genre\n",
    "lyrics = {}\n",
    "for genre in df_plot.genre.unique().tolist():\n",
    "    lyrics[genre] = ' '\n",
    "    for row in (df_plot[df_plot['genre'] == genre].index):\n",
    "        lyrics[genre] = lyrics[genre] + ' ' + df_plot.loc[row, 'lyric']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for genre, lyric in lyrics.items():\n",
    "    wordcloud = WordCloud(max_font_size=60).generate(lyric)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    fig.suptitle(genre, fontsize=24)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In thoes word cloud, word 'love' is almost the most frequent one in each genre. And word 'life', 'know' and 'time' etc. are frequently used as well.\n",
    "But there are also some differences among those genres. For example, in jazz, word 'heart' used more that other genre,\n",
    "and there are more dirty words in hip-hop, which makes sense.\n",
    "After exploring all the lyrics, we can make a conclusion that most of the lyrics have some words in common, but depending on what kind of music they are, they do have unique words. Based on this results, we can make genre prediction in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference: \n",
    "Raschka, S. (2015). Python machine learning. Packt Publishing Ltd.\n",
    "\n",
    "https://www.kaggle.com/carrie1/drug-of-choice-by-genre-using-song-lyrics\n",
    "\n",
    "https://github.com/eclarson/MachineLearningNotebooks/tree/master/data\n",
    "\n",
    "https://amueller.github.io/word_cloud/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
